# 强化学习理论基础-算法-带基线的REINFORCE算法

我们在文章[《带基线的策略梯度定理》](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(2)定理(4)带基线的策略梯度定理.html)中已经讨论和证明了如下带基线的策略梯度定理：


<div class="math">

$$
\begin{aligned}
  \frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} &= \frac{1 - \gamma^n}{1 - \gamma} \mathbb{E}_{S \sim d(\cdot)} \left [\mathbb{E}_{A \sim \pi(\cdot \mid S; \boldsymbol{\theta})} \left[ \frac{\partial \ln \pi(A \mid S; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot \left ( Q_{\pi}(S, A) - b \right ) \right]\right]
\end{aligned} \tag {0.1}
$$

</div>

其中 $b$ 是基线（baseline），可以是任意一个和动作 $a$ 无关的函数。

我们在文章[《REINFORCE算法》](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(3)算法(4)REINFORCE算法.html)中讨论了一种近似策略梯度的方法，即使用时刻 $t$ 之后汇报 $u_t$ 近似动作价值函数 $Q_{\pi}(s_t, a_t)$。进而得到近似随机梯度

<div class="math">

$$
\begin{aligned}
   \tilde{\boldsymbol{g}}(s_t, a_t; \boldsymbol{\theta}) &= \frac{\partial \ln \pi(a_t \mid s_t; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot u_t
\end{aligned} \tag{0.2}
$$

</div>

## 1. 推导

我们使用状态价值函数 $V_{\pi}(s)$ 作为基线，来改进REINFORCE算法。至于为什么选择状态价值函数作为基线，简单解释就是如果基线函数如果和动作价值函数 $Q_{\pi}(s, a)$ 关于动作 $a$ 的期望越接近，那么训练过程中的方差就会越小，这个结论我具体没有推导过，我们暂且认为是对的。

那么我们可以将策略梯度的无偏估计写成

<div class="math">

$$
\begin{aligned}
  \boldsymbol g(s, a; \boldsymbol{\theta}) &\triangleq \frac{\partial \ln \pi(a \mid s; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot (Q_{\pi}(s, a) - V_{\pi}(s))
\end{aligned} \tag{1.1}
$$

</div>


这里我们使用一个神经网络 $v(s; \boldsymbol{\omega})$ 来近似状态价值函数 $V_{\pi}(s)$ ，然后依然使用回报 $u$ 来进替代 $Q_{\pi}(s, a)$。因此我们使用蒙特卡洛法近似得到随机近似梯度

<div class="math">

$$
\begin{aligned}
   \tilde{\boldsymbol{g}}(s_t, a_t; \boldsymbol{\theta}) &= \frac{\partial \ln \pi(a_t \mid s_t; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot (u_t - v(s_t; \boldsymbol{\omega}))
\end{aligned} \tag{1.2}
$$

</div>

### 1.1. 训练策略网络

我们使用上述随机近似梯度，就可以用梯度上升法来更新策略网络的参数 $\boldsymbol{\theta}$：

<div class="math">

$$
\begin{aligned}
   \boldsymbol{\theta} &\leftarrow \boldsymbol{\theta} + \beta \cdot \tilde{\boldsymbol{g}}(s_t, a_t; \boldsymbol{\theta})
\end{aligned} \tag{1.3}
$$

</div>

注意，回忆一下文章[《REINFORCE算法》](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(3)算法(4)REINFORCE算法.html)中的推导，上述更新方式同样是简化版本

### 1.2. 训练价值网络

根据定义，状态价值函数 $V_{\pi}(s_t)$ 回报 $U_t$ 的期望

<div class="math">

$$
\begin{aligned}
   V_{\pi}(s_t) &= \mathbb{E}[U_t \mid S_t = s_t]
\end{aligned} \tag{1.4}
$$

</div>

价值网络 $v(s_t; \boldsymbol{\omega})$ 用来拟合状态价值函数 $V_{\pi}(s_t)$。我们希望它尽可能接近回报 $U_t$ 的期望，于是我们用回报 $U_t$ 的观测结果 $u_t$ 来构架如下代价函数

<div class="math">

$$
\begin{aligned}
   L(\boldsymbol{\omega}) &= \frac{1}{2n} \sum_{t=1}^{n} \left( v(s_t; \boldsymbol{\omega}) - u_t \right)^2
\end{aligned} \tag{1.5}
$$

</div>

于是可以得到损失函数的梯度

<div class="math">

$$
\begin{aligned}
   \nabla_{\boldsymbol{\omega}} L(\boldsymbol{\omega}) &= \frac{1}{n} \sum_{t=1}^{n} \left( v(s_t; \boldsymbol{\omega}) - u_t \right) \cdot \nabla v(s_t; \boldsymbol{\omega})
\end{aligned} \tag{1.6}
$$

</div>

于是可以使用梯度下降法更新价值网络的参数 $\boldsymbol{\omega}$：

<div class="math">

$$
\begin{aligned}
  \boldsymbol{\omega} &\leftarrow \boldsymbol{\omega} - \alpha \cdot \nabla_{\boldsymbol{\omega}} L(\boldsymbol{\omega})
\end{aligned} \tag{1.7}
$$

</div>


## 2. 训练流程

我们把当前策略网络和价值网络的参数分别记为 $\boldsymbol{\theta}_{now}$ 和 $\boldsymbol{\omega}_{now}$ 。训练过程如下

- 使用策略网络$ \pi(a \mid s; \boldsymbol{\theta}_{now})$ 控制智能体和环境交互并完成一个episode，得到一条轨迹

<div class="math">

$$
\begin{aligned}
  s_1, a_1, r_1, \space s_2, a_2, r_2, \space \ldots, \space s_n, a_n, r_n
\end{aligned} \tag{2.1}
$$

</div>

- 计算所有的回报 $u_t$

<div class="math">

$$
\begin{aligned}
  u_t &= r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots = \sum_{k=t}^{n} \gamma^{k-t} \cdot r_k, \space \forall t = 1, 2, \ldots, n
\end{aligned} \tag{2.2}
$$

</div>

- 计算当前价值网络的预测值

<div class="math">

$$
\begin{aligned}
  \hat v_t &=v(s_t; \boldsymbol{\omega}_{now}), \space \forall t = 1, 2, \ldots, n
\end{aligned} \tag{2.3}
$$

</div>

- 计算误差

<div class="math">

$$
\begin{aligned}
  \delta_t &= \hat v_t - u_t, \space \forall t = 1, 2, \ldots, n
\end{aligned} \tag{2.4}
$$

</div>

- 通过反向传播计算价值网络的梯度

<div class="math">

$$
\begin{aligned}
  \nabla_{\boldsymbol{\omega}} v(s_t; \boldsymbol{\omega}_{now}), \space \forall t = 1, 2, \ldots, n
\end{aligned} \tag{2.5}
$$

</div>

- 更新价值网络的参数

<div class="math">

$$
\begin{aligned}
  \boldsymbol{\omega}_{new} &\leftarrow \boldsymbol{\omega}_{now} - \alpha \cdot \frac{1}{n} \sum_{t=1}^{n} \delta_t \cdot \nabla_{\boldsymbol{\omega}} v(s_t; \boldsymbol{\omega}_{now})
\end{aligned} \tag{2.6}
$$

</div>

- 通过反向传播计算近似策略梯度

<div class="math">

$$
\begin{aligned}
  \tilde{\boldsymbol{g}}(s_t, a_t; \boldsymbol{\theta}_{now}) &= \frac{\partial \ln \pi(a_t \mid s_t; \boldsymbol{\theta}_{now})}{\partial \boldsymbol{\theta}} \cdot (u_t - v(s_t; \boldsymbol{\omega}_{now})) \\
  &= -\frac{\partial \ln \pi(a_t \mid s_t; \boldsymbol{\theta}_{now})}{\partial \boldsymbol{\theta}} \cdot \delta_t
\end{aligned}, \space \forall t = 1, 2, \ldots, n \tag{2.7}
$$

</div>

- 更新策略网络的参数

<div class="math">

$$
\begin{aligned}
  \boldsymbol{\theta}_{new} &\leftarrow \boldsymbol{\theta}_{now} + \beta \cdot \sum_{t=1}^{n} \gamma^{t-1} \cdot \tilde{\boldsymbol{g}}(s_t, a_t; \boldsymbol{\theta}_{now}) \\
  &\leftarrow \boldsymbol{\theta}_{now} - \beta \cdot \sum_{t=1}^{n} \gamma^{t-1} \cdot \frac{\partial \ln \pi(a_t \mid s_t; \boldsymbol{\theta}_{now})}{\partial \boldsymbol{\theta}} \cdot \delta_t
\end{aligned} \tag{2.8}
$$

</div>

未经允许，禁止转载。
