# 强化学习理论基础-算法-SARSA算法

前面我们介绍了如何用TD算法训练DQN，具体内容参见[《使用TD算法训练DQN》](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(3)算法(1)使用TD算法训练DQN.html)和[《使用TD算法训练DQN方法改进》](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(3)算法(2)使用TD算法训练DQN方法.html)，该方法被称为Q学习算法，根据介绍我们不难发现，Q学习算法的目标是去近似一个最优动作价值函数 $Q^*(s, a)$ ，其训练过程通常是利用一个四元组 $s_t, a_t, r_t, s_{t+1}$ 来更新学习参数，这个过程是不依赖产生四元组的具体策略的。因此Q学习属于 **异策略（Off Policy）** 算法，可以使用经验回放。

SARSA算法虽然训练过程和Q学习算法十分相似，可是它却是一种 **同策略（On Policy）** 算法。算法名称SARSA是“状态-动作-奖励-状态-动作（State-Action-Reward-State-Action）”的缩写，表明它利用一个“五元组”进行训练，即 $(s_t, a_t, r_t, s_{t+1}, \tilde{a}_{t+1})$ ，其中 $s_t$ 表示当前状态， $a_t$ 表示应用策略 $\pi$ 根据当前状态 $s_t$ 选择的动作， $r_t$ 表示在状态 $s_t$ 下执行动作 $a_t$ 后获得的奖励， $s_{t+1}$ 表示执行动作 $a_t$ 后到达的下一个状态， $\tilde{a}_{t+1}$ 表示在状态 $s_{t+1}$ 下 **模拟** 应用策略 $\pi$ 选择的动作。因此，想要得到这五元组，离不开当前策略 $\pi$ ，因此SARSA算法是 **同策略（On Policy）** 算法。

## 1. 推导

和Q学习算法一样，SARSA算法也是利用最优Bellman方程通过蒙特卡洛法进行近似然后利用TD算法进行更新。

假定当前状态是$s_t$，智能体执行动作$a_t$，获得奖励$r_t$，并转移到下一个状态$s_{t+1}$。在状态$s_{t+1}$下，智能体根据当前策略$\pi$（模拟）选择动作$\tilde{a}_{t+1}$。因此，根据最优Bellman方程，可以得到TD目标：

<div class="math">

$$
\hat y_t = r_t + \gamma \cdot q(s_{t+1}, \tilde{a}_{t+1}; \boldsymbol{\omega}) \tag{1.1}
$$

</div>

因为要通过学习使得$q(s_t, a_t; \boldsymbol{\omega})$尽可能接近$\hat y_t$，因此可以定义损失函数

<div class="math">

$$
L(\boldsymbol{\omega}) = \frac{1}{2} \left( q(s_t, a_t; \boldsymbol{\omega}) - \hat y_t \right)^2 \tag{1.2}
$$

</div>

同样的我们还是在假设$\hat y_t$是一个常数的情况下（**我猜**是因为在引入**目标网络**以后，$\hat y_t$不再随$\boldsymbol{\omega}$变化）求$L(\boldsymbol{\omega})$对$\boldsymbol{\omega}$的梯度

<div class="math">

$$
\begin{aligned}
  \nabla_{\boldsymbol{\omega}} L(\boldsymbol{\omega}) &= \left( q(s_t, a_t; \boldsymbol{\omega}) - \hat y_t \right) \nabla_{\boldsymbol{\omega}} q(s_t, a_t; \boldsymbol{\omega}) \\
  &= \delta_t \nabla_{\boldsymbol{\omega}} q(s_t, a_t; \boldsymbol{\omega})
\end{aligned}
$$

</div>

因此应用梯度下降法，得到了SARSA算法的更新公式：

<div class="math">

$$
\boldsymbol{\omega} \leftarrow \boldsymbol{\omega} - \alpha \cdot \delta_t \nabla_{\boldsymbol{\omega}} q(s_t, a_t; \boldsymbol{\omega}) \tag{1.3}
$$

</div>

## 2. 训练过程

1. 观测到当前状态$s_t$，根绝当前策略做随机抽样：$a_t \sim \pi(\cdot \mid s_t)$。

2. 用当前的价值网络计算价值

<div class="math">

$$
\hat q_t = q(s_t, a_t; \boldsymbol{\omega}_{now}) \tag{2.1}
$$

</div>

3. 智能体执行动作$a_t$，获得奖励$r_t$，并转移到下一个状态$s_{t+1}$。

4. 在状态$s_{t+1}$下，智能体根据当前策略$\pi$（模拟）选择动作$\tilde{a}_{t+1}$。

5. 用当前的价值网络计算价值

<div class="math">

$$
\hat q_{t+1} = q(s_{t+1}, \tilde{a}_{t+1}; \boldsymbol{\omega}_{now}) \tag{2.2}
$$

</div>

6. 计算TD目标和TD误差

<div class="math">

$$
\begin{aligned}
\hat y_t &= r_t + \gamma \cdot \hat q_{t+1} \\
\delta_t &= \hat q_t - \hat y_t
\end{aligned} \tag{2.3}
$$

</div>

7. 对价值网络进行反向传播计算关于 $\boldsymbol{\omega}_{now}$ 的梯度 $\nabla_{\boldsymbol{\omega}_{now}} q(s_t, a_t; \boldsymbol{\omega}_{now})$ 

8. 更新参数

<div class="math">

$$
\boldsymbol{\omega}_{new} \leftarrow \boldsymbol{\omega}_{now} - \alpha \cdot \delta_t \cdot \nabla_{\boldsymbol{\omega}_{now}} q(s_t, a_t; \boldsymbol{\omega}_{now}) \tag{2.4}
$$

</div>


未经允许，禁止转载。
