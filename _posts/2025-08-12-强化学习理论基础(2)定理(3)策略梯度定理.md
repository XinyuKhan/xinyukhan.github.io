# 强化学习理论基础-定理-策略梯度定理

## 1. 定理

$$
\begin{aligned}
  \frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} &= \frac{1 - \gamma^n}{1 - \gamma} \mathbb{E}_{S \sim d(\cdot)} \left [\mathbb{E}_{A \sim \pi(\cdot | S; \boldsymbol{\theta})} \left[ \frac{\partial \ln \pi(A | S; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_{\pi}(S, A) \right]\right]
\end{aligned} \tag {1}
$$

其中$J(\boldsymbol{\theta})$是代价函数，上述定理成立的前提是随机状态$S$服从分布马尔可夫链的稳态分布$d(\cdot)$，可以通过上述定理求得代价函数的梯度,然后利用梯度法求解如下问题：

$$
\begin{aligned}
\max_{\boldsymbol{\theta}} J(\boldsymbol{\theta})
\end{aligned}
$$

其中$J$是状态值函数的期望

$$
\begin{aligned}
J(\boldsymbol{\theta}) &= \mathbb{E}_{S \sim d(\cdot)}\left[V_\pi(S)\right] \\
\end{aligned}
$$

求解过程为

$$
\begin{aligned}
\boldsymbol{\theta}_{new} & \leftarrow \boldsymbol{\theta}_{now} + \beta \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}_{now}) \\
\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}_{now}) &\triangleq \frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \bigg|_{\boldsymbol{\theta} = \boldsymbol{\theta}_{now}}
\end{aligned} \tag{1.1}
$$

## 2. 证明

### 2.1. 递归公式

$$
\begin{aligned}
\frac{\partial V_\pi(s)}{\partial \boldsymbol{\theta}} &= \mathbb{E}_{A \sim \pi(\cdot | s; \boldsymbol{\theta})}\left[\frac{\partial \ln \pi(A | s; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s, A) + \gamma \mathbb{E}_{S' \sim p(\cdot | s, A)}\left[\frac{\partial V_\pi(S')}{\partial \boldsymbol{\theta}}\right]\right]
\end{aligned} \tag {2.1}
$$

#### 证明

根据定义有

$$
\begin{aligned}
V_\pi(s) &=\mathbb{E}_{A \sim \pi(\cdot | s; \boldsymbol{\theta})}\left[Q_\pi(s, A)\right] \\
&= \sum_{a \in \mathcal{A}} \pi(a | s; \boldsymbol{\theta}) \cdot Q_\pi(s, a)
\end{aligned}
$$

那么对其求$\boldsymbol{\theta}$的梯度有

$$
\begin{aligned}
\frac{\partial V_\pi(s)}{\partial \boldsymbol{\theta}} &= \frac{\partial}{\partial \boldsymbol{\theta}} \mathbb{E}_{A \sim \pi(\cdot | s; \boldsymbol{\theta})}\left[Q_\pi(s, A)\right] \\
&= \frac{\partial}{\partial \boldsymbol{\theta}} \sum_{a \in \mathcal{A}} \pi(a | s; \boldsymbol{\theta}) \cdot Q_\pi(s, a) \\
&= \sum_{a \in \mathcal{A}} \frac{\partial \pi(a | s; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s, a) + 
   \sum_{a \in \mathcal{A}} \pi(a | s; \boldsymbol{\theta}) \cdot \frac{\partial Q_\pi(s, a)}{\partial \boldsymbol{\theta}} \\
&= \sum_{a \in \mathcal{A}} \pi(a | s; \boldsymbol{\theta}) \cdot \frac{1}{\pi(a | s; \boldsymbol{\theta})} \frac{\partial \pi(a | s; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s, a) + 
   \sum_{a \in \mathcal{A}} \pi(a | s; \boldsymbol{\theta}) \cdot \frac{\partial Q_\pi(s, a)}{\partial \boldsymbol{\theta}} \\
&= \mathbb{E}_{A \sim \pi(\cdot | s; \boldsymbol{\theta})}\left[\frac{\partial \ln \pi(A | s; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s, A)+\underbrace{\frac{\partial Q_\pi(s, A)}{\partial \boldsymbol{\theta}}}_{\text{2.1.2}}\right]
\end{aligned} \tag{2.1.1}
$$

下面对其中的2.1.2部分进行进一步展开，假设$R$是关于状态$s$、$a$和后续状态$s'$的函数，应用[Bellman方程](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(2)定理(1)Bellman方程.html)有

$$
\begin{aligned}
Q_\pi(s, a) &= \mathbb{E}_{S' \sim p(\cdot | s, a)}\left[R(s, a, S') + \gamma V_\pi(S')\right] \\
&= \sum_{s' \in \mathcal{S}} \underbrace{p(s' | s, a) \cdot R(s, a, s')}_{\text{2.1.3}} + \gamma \sum_{s' \in \mathcal{S}} \underbrace{p(s' | s, a) \cdot V_\pi(s')}_{\text{2.1.4}}
\end{aligned} \tag{2.1.2}
$$

其中$p(\cdot | s, a)$是状态$s$和动作$a$下后续状态的转移概率分布，$R$是根据状态$s$、动作$a$和后续状态$s'$获取的奖励，这些都只跟环境和奖励设计有关，跟策略本身无关，因此2.1.2中的2.1.3部分对于$\boldsymbol{\theta}$的梯度为$\boldsymbol{0}$，2.1.4部分的状态转移条件概率函数可以提出来，于是有

$$
\begin{aligned}
\frac{\partial Q_\pi(s, a)}{\partial \boldsymbol{\theta}} &= \sum_{s' \in \mathcal{S}} \frac{\partial}{\partial \boldsymbol{\theta}}\left[p(s' | s, a) \cdot R(s, a, s') \right] + \gamma \sum_{s' \in \mathcal{S}} \frac{\partial}{\partial \boldsymbol{\theta}}\left[p(s' | s, a) \cdot V_\pi(s')\right] \\
&= \gamma \sum_{s' \in \mathcal{S}} p(s' | s, a) \cdot \frac{\partial V_\pi(s')}{\partial \boldsymbol{\theta}} \\
&= \gamma \mathbb{E}_{S' \sim p(\cdot | s, a)}\left[\frac{\partial V_\pi(S')}{\partial \boldsymbol{\theta}}\right]
\end{aligned} \tag{2.1.5}
$$

将其代入2.1.1公式2.1得证

### 2.2. 具有稳态分布的马尔可夫链的状态转移期望引理

如果$d(\cdot)$是马尔可夫链的稳态分布，那么对于函数$f(S')$，满足

$$
\begin{aligned}
\mathbb{E}_{S \sim d(\cdot)}\left[\mathbb{E}_{A \sim \pi(\cdot | S; \boldsymbol{\theta})}\left[\mathbb{E}_{S' \sim p(\cdot | S, A)}\left[f(S')\right]\right]\right] &= \mathbb{E}_{S' \sim d(\cdot)}\left[f(S')\right]
\end{aligned} \tag{2.2}
$$

#### 证明

将等式左侧展开

$$
\begin{aligned}
\mathbb{E}_{S \sim d(\cdot)}\left[\mathbb{E}_{A \sim \pi(\cdot | S; \boldsymbol{\theta})}\left[\mathbb{E}_{S' \sim p(\cdot | S, A)}\left[f(S')\right]\right]\right] &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a | s; \boldsymbol{\theta}) \sum_{s' \in \mathcal{S}} p(s' | s, a) f(s') \\
&= \sum_{s' \in \mathcal{S}} f(s') \underbrace{\sum_{s \in \mathcal{S}} \sum_{a \in \mathcal{A}} p(s' | s, a) \pi(a | s; \boldsymbol{\theta}) d(s)}_{\text{2.2.2}} \\
\end{aligned} \tag{2.2.1}
$$

根据[马尔可夫链的稳态分布](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(2)定理(2)马尔可夫链的稳态分布.html)，2.2.2

$$
\begin{aligned}
(2.2.2) &= d(s') \\
\end{aligned}
$$

代入2.2.1有

$$
\begin{aligned}
\mathbb{E}_{S \sim d(\cdot)}\left[\mathbb{E}_{A \sim \pi(\cdot | S; \boldsymbol{\theta})}\left[\mathbb{E}_{S' \sim p(\cdot | S, A)}\left[f(S')\right]\right]\right] &= \sum_{s' \in \mathcal{S}} f(s') d(s') \\
&= \mathbb{E}_{S' \sim d(\cdot)}\left[f(S')\right]
\end{aligned} \tag{2.2.1}
$$


### 2.3 迭代公式的累加展开

重新审视2.1，假定在一个Episode中的状态以$S_1$开始，经过$S_2$、$S_3$、...、$S_{t+1}$，其中的动作为$A_1$、$A_2$、...、$A_t$，从状态$S_1$开始，利用公式2.1的递归可以得到

$$
\begin{aligned}
\frac{\partial V_\pi(S_1)}{\partial \boldsymbol{\theta}} &= \mathbb{E}_{A_1}\left[\underbrace{\frac{\partial \ln \pi(A_1 | S_1; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} Q_\pi(S_1, A_1)}_{g(S_1, A_1;\boldsymbol{\theta})} + \gamma \mathbb{E}_{S_2}\left[\frac{\partial V_\pi(S_2)}{\partial \boldsymbol{\theta}}\right]\right] \\
=& \mathbb{E}_{A_1}\left[g(S_1, A_1;\boldsymbol{\theta})\right] + \gamma \mathbb{E}_{A_1,S_2}\left[\frac{\partial V_\pi(S_2)}{\partial \boldsymbol{\theta}}\right] \\
=& \mathbb{E}_{A_1}\left[g(S_1, A_1;\boldsymbol{\theta})\right] + \gamma \mathbb{E}_{A_1,S_2}\left[\mathbb{E}_{A_2}\left[\underbrace{\frac{\partial \ln \pi(A_2 | S_2; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} Q_\pi(S_2, A_2)}_{g(S_2, A_2;\boldsymbol{\theta})} + \gamma \mathbb{E}_{S_3}\left[\frac{\partial V_\pi(S_3)}{\partial \boldsymbol{\theta}}\right]\right]\right] \\
=& \mathbb{E}_{A_1}\left[g(S_1, A_1;\boldsymbol{\theta})\right] + \gamma \mathbb{E}_{A_1,S_2}\left[g(S_2, A_2;\boldsymbol{\theta})\right] + \gamma^2 \mathbb{E}_{A_1,S_2,A_2,S_3}\left[\frac{\partial V_\pi(S_3)}{\partial \boldsymbol{\theta}}\right] \\
=& \mathbb{E}_{A_1}\left[g(S_1, A_1;\boldsymbol{\theta})\right] \\
 &+ \gamma \mathbb{E}_{A_1,S_2,A_2}\left[g(S_2, A_2;\boldsymbol{\theta})\right] \\
 &+ \gamma^2 \mathbb{E}_{A_1,S_2,A_2,S_3,A_3}\left[g(S_3, A_3;\boldsymbol{\theta})\right] \\
 &+ \cdots \\
 &+ \gamma^{n-1} \mathbb{E}_{A_1,S_2,\ldots,S_n,A_n}\left[g(S_n, A_n;\boldsymbol{\theta})\right] \\
 &+ \gamma^n \mathbb{E}_{A_1,S_2,\ldots,S_n,A_n,S_{n+1}}\left[\underbrace{\frac{\partial V_\pi(S_{n+1})}{\partial \boldsymbol{\theta}}}_{\text{=0}}\right] \\
\end{aligned} \tag{2.3}
$$

由于$S_{n+1}$是终止状态，因此$V_\pi(S_{n+1})=0$，所以$\frac{\partial V_\pi(S_{n+1})}{\partial \boldsymbol{\theta}}=0$

根据定义

$$
\begin{aligned}
\frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} =& \frac{\partial}{\partial \boldsymbol{\theta}} \mathbb{E}_{S_1 \sim d(\cdot)}\left[V_\pi(S_1)\right] \\
=& \mathbb{E}_{S_1 \sim d(\cdot)}\left[\frac{\partial V_\pi(S_1)}{\partial \boldsymbol{\theta}}\right] \\
=& \mathbb{E}_{S_1,A_1}\left[g(S_1, A_1;\boldsymbol{\theta})\right] \\
 &+ \gamma \mathbb{E}_{S_1,A_1,S_2,A_2}\left[g(S_2, A_2;\boldsymbol{\theta})\right] \\
 &+ \gamma^2 \mathbb{E}_{S_1,A_1,S_2,A_2,S_3,A_3}\left[g(S_3, A_3;\boldsymbol{\theta})\right] \\
 &+ \cdots \\
 &+ \gamma^{n-1} \mathbb{E}_{S_1,A_1,S_2,\ldots,S_n,A_n}\left[g(S_n, A_n;\boldsymbol{\theta})\right] \\
\end{aligned}
$$

对上式中的每一项多次应用2.2中的具有稳态分布的马尔可夫链的状态转移期望引理，可以得到

$$
\begin{aligned}
\frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}
=& \mathbb{E}_{S_1,A_1}\left[g(S_1, A_1;\boldsymbol{\theta})\right] \\
 &+ \gamma \mathbb{E}_{S_1,A_1,S_2,A_2}\left[g(S_2, A_2;\boldsymbol{\theta})\right] \\
 &+ \gamma^2 \mathbb{E}_{S_1,A_1,S_2,A_2,S_3,A_3}\left[g(S_3, A_3;\boldsymbol{\theta})\right] \\
 &+ \cdots \\
 &+ \gamma^{n-1} \mathbb{E}_{S_1,A_1,S_2,\ldots,S_n,A_n}\left[g(S_n, A_n;\boldsymbol{\theta})\right] \\
=& \mathbb{E}_{S_1,A_1}\left[g(S_1, A_1;\boldsymbol{\theta})\right] \\
 &+ \gamma \mathbb{E}_{S_2,A_2}\left[g(S_2, A_2;\boldsymbol{\theta})\right] \\
 &+ \gamma^2 \mathbb{E}_{S_3,A_3}\left[g(S_3, A_3;\boldsymbol{\theta})\right] \\
 &+ \cdots \\
 &+ \gamma^{n-1} \mathbb{E}_{S_n,A_n}\left[g(S_n, A_n;\boldsymbol{\theta})\right] \\
=& \left(1 + \gamma + \gamma^2 + \cdots + \gamma^{n-1}\right) \mathbb{E}_{S \sim d(\cdot)}\left[\mathbb{E}_{A \sim \pi(\cdot | S; \boldsymbol{\theta})}\left[g(S, A;\boldsymbol{\theta})\right]\right] \\
=& \frac{1 - \gamma^n}{1 - \gamma} \mathbb{E}_{S \sim d(\cdot)}\left[\mathbb{E}_{A \sim \pi(\cdot | S; \boldsymbol{\theta})}\left[ \frac{\partial \ln \pi(A | S; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_{\pi}(S, A) \right]\right]
\end{aligned}
$$

证明完毕
