# 强化学习理论基础-算法-使用TD算法训练DQN

## 1. 算法概述

最优动作价值函数$Q_*$定义如下

$$
\begin{aligned}
  Q_*(s, a) & = \max_\pi Q_\pi(s, a), \forall s \in \mathcal{S}, a \in \mathcal{A} \\
  &= \mathbb{E}\left[ U_t | S_t = s, A_t = a \right] \\
  &= \mathbb{E}\left[ R_t + \gamma U_{t+1} | S_t = s, A_t = a \right]
\end{aligned} \tag{1.1}
$$

假如知道最优动作-价值函数$Q_*$的表达式，就可以获得如下策略

$$
\begin{aligned}
  a^*(s) & = \arg\max_{a \in \mathcal{A}} Q_*(s, a), \forall s \in \mathcal{S} \\
\end{aligned} \tag{1.2}
$$

Q学习就是通过一个参数方法$Q(a,s;\boldsymbol{\omega})$近似最优动作-价值函数$Q_*$，如果这个参数方法使用的是一个深度神经网络，那就称之为DQN（Deep Q-Network）。

## 2. 使用TD（Temporal Difference）方法训练DQN

### 2.1. 算法推导

根据最优Bellman方程（详情参考[Bellman方程](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(2)定理(1)Bellman方程.html)），我们有

$$
\begin{aligned}
  Q_*(s_t, a_t) & = \mathbb{E}_{S_{t+1} \sim p(\cdot \mid s_t, a_t)}\left[ R_t + \gamma \max_{A \in \mathcal{A}} Q_*(S_{t+1}, A) | S_t = s_t, A_t = a_t \right] \\
\end{aligned} \tag{2.1.1}
$$

公式(2.1.1)右侧的期望可以使用蒙特卡洛法进行近似，当前状态为$s_t$，采取的动作为$a_t$，环境根据状态转移函数$p(\cdot \mid s_t, a_t)$迁移到下一个状态$s_{t+1}$，并且得到一个奖励$r_t$，因此我们可以得到，于是得到一个四元组

$$(s_t, a_t, r_t, s_{t+1}) \tag{2.1.2}$$


于是可以计算出


$$r_t + \gamma \max_{a \in \mathcal{A}} Q_*(s_{t+1}, a) \tag{2.1.3}$$

将其看作对公式(2.1.1)的蒙特卡洛近似，我们可以得到

$$
\begin{aligned}
  Q_*(s_t, a_t) & \approx r_t + \gamma \max_{a \in \mathcal{A}} Q_*(s_{t+1}, a) \\
\end{aligned} \tag{2.1.4}
$$

下面用DQN根据当前的四元组对$Q_*$进行预测

$$
\begin{aligned}
  \hat q_t &= Q(s_t, a_t; \boldsymbol{\omega}) \\
  \hat y_t &= r_t + \gamma \max_{a \in \mathcal{A}} Q(s_{t+1}, a; \boldsymbol{\omega}) \\
\end{aligned} \tag{2.1.5}
$$

其中$\hat q_t$是对 $Q_{\ast}(s_t, a_t)$ 的预测，$\hat y_t$是对$r_t + \gamma \max_{a \in \mathcal{A}} Q_{\ast}(s_{t+1}, a)$的预测。$\hat y_t$比$\hat q_t$更可信，因为它部分（$r_t$）基于事实观测，据此构建损失函数

$$
L(\boldsymbol{\omega}) = \frac{1}{2}\left[ \hat y_t - \hat q_t \right]^2 \tag{2.1.6}
$$

计算$L$关于$\boldsymbol{\omega}$的梯度，注意，这里假装$\hat y_t$是常数 **（此处不甚明白）**。

$$
\begin{aligned}
\nabla_{\boldsymbol{\omega}} L(\boldsymbol{\omega}) &= \left[ \hat y_t - \hat q_t \right] \nabla_{\boldsymbol{\omega}} \hat q_t  \\
&= \underbrace{\left[ \hat y_t - \hat q_t \right]}_{\text{TD误差}\delta_t} \nabla_{\boldsymbol{\omega}} Q(s_t, a_t; \boldsymbol{\omega})
\end{aligned} \tag{2.1.7}
$$

所以使用梯度下降法更新参数$\boldsymbol{\omega}$的方式如下

$$
\boldsymbol{\omega} \leftarrow \boldsymbol{\omega} - \alpha \cdot \delta_t \cdot \nabla_{\boldsymbol{\omega}} Q(s_t, a_t; \boldsymbol{\omega}) \tag{2.1.8}
$$


### 2.2. 训练过程

1. 获取四元组

$$
(s_t, a_t, r_t, s_{t+1}) \tag{2.2.1}
$$

2. 计算TD目标

$$
\hat y_t = r_t + \gamma \max_{a \in \mathcal{A}} Q(s_{t+1}, a; \boldsymbol{\omega}) \tag{2.2.2}
$$

3. 计算TD误差

$$
\delta_t = \hat q_t - \hat y_t \tag{2.2.3}
$$

3. 更新参数

$$
\boldsymbol{\omega} \leftarrow \boldsymbol{\omega} - \alpha \cdot \delta_t \cdot \nabla_{\boldsymbol{\omega}} Q(s_t, a_t; \boldsymbol{\omega}) \tag{2.2.4}
$$

## 3. 总结

通过使用TD算法训练DQN，我们可以使用深度神经网络来逼近最优动作-价值函数$Q_*$，并通过TD方法进行训练。具体步骤包括获取四元组、计算TD目标、计算TD误差和更新参数。DQN属于异策略（off-policy）学习方法，可以使用经验回放（experience replay）来提高样本效率和稳定性。
