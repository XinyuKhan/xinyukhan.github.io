# 强化学习理论基础-算法-使用TD算法训练DQN方法改进

## 1. 改进

### 1.1. 多步TD

单步TD算法通过当前状态和动作与下一个状态的价值进行比较来更新Q值，而多步TD算法则考虑多个时间步的奖励和价值，从而更全面地评估动作的价值。这种方法可以加速学习过程，提高策略的稳定性。

训练过程如下

1. 假设智能体和环境交互得到如下轨迹

$$
(s_1, a_1, r_1, s_2, a_2, r_2, \ldots, s_n, a_n, r_n) \tag{1.1.1}
$$

2. 选取轨迹上的一个点$t$，使用动作价值网络对其进行估计得到$\hat q_t$

$$
\hat q_t = Q(s_t, a_t; \boldsymbol{\omega}), t \in \{1, 2, \ldots, n-m\} \tag{1.1.2}
$$

3. 计算多步TD目标和多步TD误差

$$
\begin{aligned}
\hat y_t &= \sum_{i=0}^{m-1} \gamma^i r_{t+i} + \gamma^m \hat q_{t+m} \\
\delta_t &= \hat q_t - \hat y_t
\end{aligned} \tag{1.1.3}
$$

4. 更新参数

$$
\boldsymbol{\omega} \leftarrow \boldsymbol{\omega} - \alpha \cdot \delta_t \cdot \nabla_{\boldsymbol{\omega}} Q(s_t, a_t; \boldsymbol{\omega}) \tag{1.1.4}
$$

### 1.2. 优先经验回放

优先经验回放和普通经验回放的不同点在于会根据TD误差$\delta_t$的绝对值来调整样本的采样概率$p_t$，从而更频繁地采样那些具有较大TD误差的样本

$$
p_t \propto |\delta_t| + \epsilon \tag{1.2.1}
$$

或者也可以对$|\delta_t|$进行排序之后根据序号来确定样本采样概率

$$
p_t \propto \frac{1}{\text{rank}(|\delta_t|)} \tag{1.2.2}
$$

同时还会根据$p_t$动态调整不同样本的学习率

$$
\alpha_t = \frac{\alpha}{(b \cdot p_t)^\beta} \tag{1.2.3}
$$

其中$b$是经验回放中的样本总数，$\beta \in (0, 1)$是一个超参数。

### 1.3. 目标网络

引入目标网络是为了解决自举导致的偏差传播，该方法通过另一个和DQN相同结构但是不同参数的网络来计算目标值，从而减少了自举的影响。

训练过程如下：

1. 使用DQN计算动作价值函数的估计$\hat q_t$

$$
\hat q_t = Q(s_t, a_t; \boldsymbol{\omega}_{now}) \tag{1.3.1}
$$

2. 使用目标网络估计目标网络价值$\hat q_{t+1}^-$

$$
\hat q_{t+1}^- = \max_{a \in \mathcal{A}} Q(s_{t+1}, a; \boldsymbol{\omega}_{now}^-) \tag{1.3.2}
$$

3. 计算TD目标和TD误差

$$
\begin{aligned}
\hat y_t &= r_t + \gamma \hat q_{t+1}^- \\
\delta_t &= \hat q_t - \hat y_t
\end{aligned} \tag{1.3.3}
$$

4. 更新参数

$$
\boldsymbol{\omega}_{new} \leftarrow \boldsymbol{\omega}_{now} - \alpha \cdot \delta_t \cdot \nabla_{\boldsymbol{\omega}_{now}} Q(s_t, a_t; \boldsymbol{\omega}_{now}) \tag{1.3.4}
$$

5. 通过参数$\tau \in (0, 1)$对目标网络进行加权平均更新

$$
\boldsymbol{\omega}_{new}^- \leftarrow \tau \cdot \boldsymbol{\omega}_{new} + (1 - \tau) \cdot \boldsymbol{\omega}_{now}^- \tag{1.3.5}
$$

### 1.4. 双Q学习法

目标网络只解决了自举导致的偏差传播问题，但仍然存最大化导致的高估，将上述目标网络改进中的第2步中的最大化拆分成两步

1. 选择动作

$$
a_t = \arg\max_{a \in \mathcal{A}} Q(s_t, a; \boldsymbol{\omega}_{now}^-) \tag{1.4.1}
$$

2. 计算估计

$$
\hat q_{t+1}^- = \max_{a \in \mathcal{A}} Q(s_{t+1}, a; \boldsymbol{\omega}_{now}^-) \tag{1.4.2}
$$

双Q学习法的不同在于通过DQN选择动作，然后使用目标网络计算估计值，从而避免了最大化导致的高估。

训练过程如下

1. 使用DQN计算动作价值函数的估计$\hat q_t$

$$
\hat q_t = Q(s_t, a_t; \boldsymbol{\omega}_{now}) \tag{1.4.3}
$$

2. 选择动作

$$
a^* = \arg\max_{a \in \mathcal{A}} Q(s_t, a; \boldsymbol{\omega}_{now}) \tag{1.4.4}
$$

3. 计算估计

$$
\hat q_{t+1}^- = Q(s_{t+1}, a^*; \boldsymbol{\omega}_{now}^-) \tag{1.4.5}
$$

4. 计算TD目标和TD误差

$$
\begin{aligned}
\hat y_t &= r_t + \gamma \hat q_{t+1}^- \\
\delta_t &= \hat q_t - \hat y_t
\end{aligned} \tag{1.4.6}
$$

5. 更新参数

$$
\boldsymbol{\omega}_{new} \leftarrow \boldsymbol{\omega}_{now} - \alpha \cdot \delta_t \cdot \nabla_{\boldsymbol{\omega}_{now}} Q(s_t, a_t; \boldsymbol{\omega}_{now}) \tag{1.4.7}
$$

6. 计算目标网络的更新

$$
\boldsymbol{\omega}_{new}^- \leftarrow \tau \cdot \boldsymbol{\omega}_{new} + (1 - \tau) \cdot \boldsymbol{\omega}_{now}^- \tag{1.4.8}
$$

### 1.5. 对决网络

对决网络是对DQN结构的改进，具体表达式如下

$$
Q(s, a; \boldsymbol{\omega}) = V(s; \boldsymbol{\omega}^V) + D(s, a; \boldsymbol{\omega}^D) - \max_{a' \in \mathcal{A}} D(s, a'; \boldsymbol{\omega}^D) \tag{1.5.1}
$$

其训练方法和DQN完全一样

### 1.6. 噪声DQN

噪声网络的定义如下

$$
\widetilde{Q}(s, a, \boldsymbol{\xi}; \boldsymbol{\mu}, \boldsymbol{\sigma}) = Q(s, a; \boldsymbol{\mu} + \boldsymbol{\sigma} \circ  \boldsymbol{\xi}) \tag{1.6.1}
$$

TD目标的计算方式和DQN一样，只不过随机变量需要重新采样为$\xi'$

$$
\hat{y}_t = r_t + \gamma \max_{a \in \mathcal{A}} \widetilde{Q}(s_{t+1}, a, \boldsymbol{\xi}'; \boldsymbol{\mu}, \boldsymbol{\sigma}) \tag{1.6.2}
$$

损失函数为

$$
L(\boldsymbol{\mu}, \boldsymbol{\sigma}) = \frac{1}{2} \left[\widetilde{Q}(s_t, a_t, \boldsymbol{\xi}; \boldsymbol{\mu}, \boldsymbol{\sigma})  - \hat{y}_t \right]^2 \tag{1.6.3}
$$

更新参数的方式为

$$
\begin{aligned}
\boldsymbol{\mu} &\leftarrow \boldsymbol{\mu} - \alpha_\mu \cdot \nabla_{\boldsymbol{\mu}} L(\boldsymbol{\mu}, \boldsymbol{\sigma}) \\
\boldsymbol{\sigma} &\leftarrow \boldsymbol{\sigma} - \alpha_\sigma \cdot \nabla_{\boldsymbol{\sigma}} L(\boldsymbol{\mu}, \boldsymbol{\sigma})
\end{aligned} \tag{1.6.4}
$$

### 1.7. 综合应用

综合应用上述方法之后的TD算法的训练过程如下

1. 使用优先经验回放抽取一个四元组

$$
(s_t, a_t, r_t, s_{t+1})
$$

2. 使用正态分布生成噪声$\boldsymbol{\xi}，通过噪声DQN计算动作价值函数的估计

$$
\hat q_t = \widetilde{Q}(s_t, a_t, \boldsymbol{\xi}; \boldsymbol{\mu}_{now}, \boldsymbol{\sigma}_{now}) \tag{1.7.1}
$$

3. 使用噪声DQN选择动作

$$
\widetilde a_{t+1} = \arg\max_{a \in \mathcal{A}} \widetilde{Q}(s_t, a, \boldsymbol{\xi}; \boldsymbol{\mu}_{now}, \boldsymbol{\sigma}_{now}) \tag{1.7.2}
$$

4. 使用标准正态分布生成$\boldsymbol{\xi}'$，用目标网络计算价值

$$
\hat q_{t+1}^- = \widetilde{Q}(s_{t+1}, \widetilde a_{t+1}, \boldsymbol{\xi}'; \boldsymbol{\mu}_{now}^-, \boldsymbol{\sigma}_{now}^-) \tag{1.7.3}
$$

5. 计算TD目标和TD误差

$$
\begin{aligned}
\hat y_t^- &= r_t + \gamma \hat q_{t+1}^- \\
\delta_t &= \hat q_t - \hat y_t^-
\end{aligned} \tag{1.7.4}
$$

6. 更新噪声DQN的参数

$$
\begin{aligned}
\boldsymbol{\mu}_{new} &\leftarrow \boldsymbol{\mu}_{now} - \alpha_\mu \cdot \delta_t \cdot \nabla_{\boldsymbol{\mu}} \widetilde{Q}(s_t, a_t, \boldsymbol{\xi}; \boldsymbol{\mu}_{now}, \boldsymbol{\sigma}_{now}) \\
\boldsymbol{\sigma}_{new} &\leftarrow \boldsymbol{\sigma}_{now} - \alpha_\sigma \cdot \delta_t \cdot \nabla_{\boldsymbol{\sigma}} \widetilde{Q}(s_t, a_t, \boldsymbol{\xi}; \boldsymbol{\mu}_{now}, \boldsymbol{\sigma}_{now})
\end{aligned} \tag{1.7.5}
$$


7. 更新目标网络的参数

$$
\begin{aligned}
\boldsymbol{\mu}_{new}^- \leftarrow \tau \cdot \boldsymbol{\mu}_{new} + (1 - \tau) \cdot \boldsymbol{\mu}_{now}^- \\
\boldsymbol{\sigma}_{new}^- \leftarrow \tau \cdot \boldsymbol{\sigma}_{new} + (1 - \tau) \cdot \boldsymbol{\sigma}_{now}^-
\end{aligned} \tag{1.7.6}
$$