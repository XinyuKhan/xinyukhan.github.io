# 强化学习理论基础-算法-REINFORCE算法


在[《策略梯度定理》](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(2)定理(3)策略梯度定理.html)一文中我们证明推导了策略梯度定理，现在把它抄到下边以供回忆


<div class="math">

$$
\begin{aligned}
  \frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} &= \frac{1 - \gamma^n}{1 - \gamma} \mathbb{E}_{S \sim d(\cdot)} \left [\mathbb{E}_{A \sim \pi(\cdot | S; \boldsymbol{\theta})} \left[ \frac{\partial \ln \pi(A | S; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_{\pi}(S, A) \right]\right]
\end{aligned} \tag {0.1}
$$

</div>

同时我们介绍了近似策略梯度，即使用蒙特卡洛近似表示上述期望，得到一个随机策略梯度函数$g$

$$
\begin{aligned}
   \boldsymbol{g}(s, a; \boldsymbol{\theta}) &\triangleq \frac{\partial \ln \pi(a | s; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_{\pi}(s, a)
\end{aligned} \tag{0.2}
$$


接下来我们使用上述方法来推导REINFORCE算法。

## 1. 推导

回忆[《策略梯度定理》](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(2)定理(3)策略梯度定理.html)中的公式(2.3.1)，抄在下方


$$
\begin{aligned}
\frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} =& \frac{\partial}{\partial \boldsymbol{\theta}} \mathbb{E}_{S_1 \sim d(\cdot)}\left[V_\pi(S_1)\right] \\
=& \mathbb{E}_{S_1 \sim d(\cdot)}\left[\frac{\partial V_\pi(S_1)}{\partial \boldsymbol{\theta}}\right] \\
=& \mathbb{E}_{S_1,A_1}\left[\boldsymbol{g}(S_1, A_1;\boldsymbol{\theta})\right] \\
 &+ \gamma \mathbb{E}_{S_1,A_1,S_2,A_2}\left[\boldsymbol{g}(S_2, A_2;\boldsymbol{\theta})\right] \\
 &+ \gamma^2 \mathbb{E}_{S_1,A_1,S_2,A_2,S_3,A_3}\left[\boldsymbol{g}(S_3, A_3;\boldsymbol{\theta})\right] \\
 &+ \cdots \\
 &+ \gamma^{n-1} \mathbb{E}_{S_1,A_1,S_2,\ldots,S_n,A_n}\left[\boldsymbol{g}(S_n, A_n;\boldsymbol{\theta})\right] \\
\end{aligned} \tag{1.1}
$$

对上述期望做蒙特卡洛近似，假设我们从状态$s_1$开始，应用策略 $\pi(a \mid s; \boldsymbol{\theta}_{now})$控制智能体和环境交互，产生如下轨迹

$$
\begin{aligned}
s_1, a_1, r_1,\space\space s_2, a_2, r_2, \space\space \ldots, \space\space s_n, a_n, r_n
\end{aligned} \tag{1.2}
$$

使用上述轨迹对公式(1.1)进行蒙特卡洛近似，得到

$$
\begin{aligned}
\frac{\partial J(\boldsymbol{\theta}_{now})}{\partial \boldsymbol{\theta}_{now}} \approx g(s_1, a_1; \boldsymbol{\theta}_{now}) + \gamma g(s_2, a_2; \boldsymbol{\theta}_{now}) + \gamma^2 g(s_3, a_3; \boldsymbol{\theta}_{now}) + \cdots + \gamma^{n-1} g(s_n, a_n; \boldsymbol{\theta}_{now})
\end{aligned} \tag{1.3}
$$

根据定义有

$$
\begin{aligned}
u_t & = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots + \gamma^{n-t} r_n = \sum_{i=t}^{n} \gamma^{i-t} r_i \\
\end{aligned} \tag{1.4}
$$

我们可以使用$u_t$近似公式(0.2)中的 $Q_{\pi}(s, a)$，有

$$
\begin{aligned}
g(s_t, a_t; \boldsymbol{\theta}_{now}) & \approx \frac{\partial \ln \pi(a_t | s_t; \boldsymbol{\theta}_{now})}{\partial \boldsymbol{\theta}_{now}} \cdot u_t \\
\end{aligned} \tag{1.5}
$$

代入公式(1.3)，我们得到

$$
\begin{aligned}
\frac{\partial J(\boldsymbol{\theta}_{now})}{\partial \boldsymbol{\theta}_{now}} & \approx \sum_{t=1}^{n} \left( \gamma^{t-1} \cdot \frac{\partial \ln \pi(a_t | s_t; \boldsymbol{\theta}_{now})}{\partial \boldsymbol{\theta}_{now}} \cdot u_t \right)
\end{aligned}
$$


于是根据随机梯度上升公式，我们更新参数

$$
\begin{aligned}
\boldsymbol{\theta}_{new} & \leftarrow \boldsymbol{\theta}_{now} + \alpha \cdot \sum_{t=1}^{n} \left( \gamma^{t-1} \cdot \frac{\partial \ln \pi(a_t | s_t; \boldsymbol{\theta}_{now})}{\partial \boldsymbol{\theta}_{now}} \cdot u_t \right)
\end{aligned}
$$

## 2. 训练方法

（略）