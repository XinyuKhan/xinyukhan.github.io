# 强化学习理论基础

接下来介绍强化学习的理论基础，本文的主要内容是明确强化学习算法中的一些背景和基本概念。

## 符号

<div class="math">

$$
\begin{aligned}
\mathcal{S} & : \text{状态空间} \\
\mathcal{A} & : \text{动作空间} \\
S & : \text{随机状态} \\
s & : \text{状态} \\
A & : \text{随机动作} \\
a & : \text{动作} \\
R & : \text{随机奖励} \\
r & : \text{奖励} \\
U & : \text{随机回报} \\
u & : \text{回报} \\
\gamma & : \text{折扣因子} \\
\pi(a|s) & : \text{随机性策略} \\
\boldsymbol{\mu}(s) & : \text{确定性策略} \\
Q_\pi(s, a) & : \text{状态-动作值函数} \\
Q_*(s, a) & : \text{最优状态-动作值函数} \\
V_\pi(s) & : \text{状态值函数} \\
V_*(s) & : \text{最优状态值函数} \\
A_\pi(s, a) & : \text{优势函数} \\
A_*(s, a) & : \text{最优优势函数} \\
\pi(a|s;\boldsymbol{\theta}) & : \text{最优策略} \\
\mu(a|s;\boldsymbol{\theta}) & : \text{最优确定性策略} \\
Q(s, a;\boldsymbol{\omega}) & : \text{深度Q网络} \\
q(s, a;\boldsymbol{\omega}) & : \text{价值网络} \\
\end{aligned}
$$

</div>

## 定义

### finite-horizon 回报:

<div class="math">

$$
\begin{aligned}
U_t & = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots + \gamma^{n-t} R_n = \sum_{i=t}^{n} \gamma^{i-t} R_i \\
u_t & = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots + \gamma^{n-t} r_n = \sum_{i=t}^{n} \gamma^{i-t} r_i \\
\end{aligned}
$$

</div>

### infinite-horizon 回报:

<div class="math">

$$
\begin{aligned}
U_t & = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots = \sum_{i=t}^{\infty} \gamma^{i-t} R_i \\
u_t & = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots = \sum_{i=t}^{\infty} \gamma^{i-t} r_i \\
\end{aligned}
$$

</div>

### 状态-动作值函数:

<div class="math">

$$
\begin{aligned}
Q_\pi(s_t, a_t) & = \mathbb{E}_{\mathcal{S}_{t+1:}, \mathcal{A}_{t+1:}}[U_t | S_t = s_t, A_t = a_t] \\
\mathcal{S}_{t+1:} &= \set{S_{t+1}, S_{t+2}, \ldots} \\
\mathcal{A}_{t+1:} &= \set{A_{t+1}, A_{t+2}, \ldots} \\
\end{aligned}
$$

</div>

### 最优状态-动作值函数:

<div class="math">

$$
\begin{aligned}
Q_*(s_t, a_t) & = \max_\pi Q_\pi(s_t, a_t), \forall s_t \in \mathcal{S}, a_t \in \mathcal{A} \\
\end{aligned}
$$

</div>

### 状态值函数:

<div class="math">

$$
\begin{aligned}
V_\pi(s_t) & = \mathbb{E}_{A\sim\pi}[Q_\pi(s_t, A)] \\
\end{aligned}
$$

</div>

### 最优状态值函数:

<div class="math">

$$
\begin{aligned}
V_*(s_t) & = \max_\pi V_\pi(s_t) \\
\end{aligned}
$$

</div>

## 定理

### [Bellman方程](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(2)定理(1)Bellman方程.html)

### [马尔可夫链的稳态分布](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(2)定理(2)马尔可夫链的稳态分布.html)

### [策略梯度定理](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(2)定理(3)策略梯度定理.html)

### [带基线的策略梯度定理](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(2)定理(4)带基线的策略梯度定理.html)

## 算法

### [使用TD算法训练DQN](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(3)算法(1)使用TD算法训练DQN.html)

### [使用TD算法训练DQN方法改进](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(3)算法(2)使用TD算法训练DQN方法改进.html)

### [SARSA算法](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(3)算法(3)SARSA算法.html)

### [REINFORCE算法](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(3)算法(4)REINFORCE算法.html)

### [Actor-Critic算法](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(3)算法(5)Actor-Critic算法.html)

### [带基线的REINFORCE算法](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(3)算法(6)带基线的REINFORCE算法.html)

### [带基线的Actor-Critic算法（A2C算法）](https://xinyukhan.github.io/2025/08/12/强化学习理论基础(3)算法(7)带基线的Actor-Critic算法（A2C算法）.html)

未经允许，禁止转载。