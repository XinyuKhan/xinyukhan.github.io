# 强化学习理论基础-公式-Bellman方程

## 1. 将$Q_\pi$表示成$Q_\pi$的递归形式

$$
\begin{aligned}
Q_\pi(s_t, a_t) & = \mathbb{E}_{S_{t+1}, A_{t+1}}[R_t + \gamma Q_\pi(S_{t+1}, A_{t+1}) | S_t = s_t, A_t = a_t] \\
\end{aligned} \tag {1}
$$

### 证明

根据定义

$$
\begin{aligned}
Q_\pi(s_t, a_t) & = \mathbb{E}_{\mathcal{S}_{t+1:}, \mathcal{A}_{t+1:}}[U_t | S_t = s_t, A_t = a_t] \\
\end{aligned} \tag {1.1}
$$
其中
$$
\begin{aligned}
U_t &= R_t + \gamma \underbrace{\sum_{k=t+1}^{n} \gamma^{k-t-1} R_k}_{=U_{t+1}} \\
&= R_t + \gamma U_{t+1} \\
\end{aligned} \tag {1.2}
$$

代入可得到

$$
\begin{aligned}
Q_\pi(s_t, a_t) & = \mathbb{E}_{\mathcal{S}_{t+1:}, \mathcal{A}_{t+1:}}[R_t + \gamma U_{t+1} | S_t = s_t, A_t = a_t] \\
& = \underbrace{\mathbb{E}_{\mathcal{S}_{t+1:}, \mathcal{A}_{t+1:}}[R_t | S_t = s_t, A_t = a_t]}_{(1.3.1)} + \gamma \underbrace{\mathbb{E}_{\mathcal{S}_{t+1:}, \mathcal{A}_{t+1:}}[U_{t+1} | S_t = s_t, A_t = a_t]}_{(1.3.2)} \\
\end{aligned} \tag {1.3}
$$

其中1.3.1中的${R_t}$是在智能体以状态${S_t}$作为输入应用策略${\pi}$得到动作${A_t}$后，环境反馈转移到状态${S_{t+1}}$的时候获得的奖励。因此其随机性来源于$S_t$，$A_t$和$S_{t+1}$，1.3.1中${S_t}和${A_t}$都已固定为${s_t}和${a_t}，因此1.3.1的随机性仅来源于${S_{t+1}}，于是有：

$$
\begin{aligned}
(1.3.1) & = \mathbb{E}_{S_{t+1}}[R_t | S_t = s_t, A_t = a_t] \\
\end{aligned} \tag {1.4}
$$

1.3.2则可以写成如下形式：

$$
\begin{aligned}
(1.3.2) & = \mathbb{E}_{S_{t+1}, A_{t+1}}[\mathbb{E}_{\mathcal{S}_{t+2:}, \mathcal{A}_{t+2:}}[U_{t+1} | S_{t+1}, A_{t+1}] | S_t = s_t, A_t = a_t] \\
& = \mathbb{E}_{S_{t+1}, A_{t+1}}[Q_\pi(S_{t+1}, A_{t+1}) | S_t = s_t, A_t = a_t] \\
\end{aligned} \tag {1.5}
$$

将(1.4)和(1.5)代入(1.3)中可得公式1。

